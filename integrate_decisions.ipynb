{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df5d32-21dd-434e-a858-1a7755e0c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae1721-938b-4556-8ce5-133a0d85a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = pd.read_csv(\"benchmark/ground_truth.csv\")\n",
    "for side in (\"source\", \"target\"):\n",
    "    benchmark[[f\"{side}_schema\", f\"{side}_relation\", f\"{side}_attribute\"]] = benchmark[side].str.split(\".\", expand=True)\n",
    "    benchmark[side] = benchmark[side].str.lower()\n",
    "benchmark[\"benchmark\"] = True\n",
    "\n",
    "gpt35_results = pd.read_csv(\"results/gpt35_results.csv\")\n",
    "gpt4_results = pd.read_csv(\"results/gpt4_results.csv\")\n",
    "baseline_results = pd.read_csv(\"results/baseline_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ac077-884f-4e9e-b14c-eba5e8053e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviate(source: str, target: str) -> str:\n",
    "    return {\n",
    "        (\"patients\", \"person\"): \"PaPe\",\n",
    "        (\"admissions\", \"visit_occurrence\"): \"AdVO\",\n",
    "        (\"prescriptions\", \"drug_exposure\"): \"PrDE\",\n",
    "        (\"admissions\", \"condition_occurrence\"): \"AdCO\",\n",
    "        (\"diagnoses_icd\", \"condition_occurrence\"): \"DiCO\",\n",
    "        (\"labevents\", \"measurement\"): \"LaMe\",\n",
    "        (\"admissions\", \"visit_detail\"): \"AdVD\",\n",
    "        (\"services\", \"visit_detail\"): \"SeVD\",\n",
    "        (\"transfers\", \"visit_detail\"): \"TrVD\",\n",
    "    }[(source.lower(), target.lower())]\n",
    "\n",
    "\n",
    "gpt35_results[\"dataset\"] = gpt35_results.apply(lambda row: abbreviate(row[\"source_relation\"], row[\"target_relation\"]), axis=\"columns\")\n",
    "gpt35_results[\"experiment_run\"] = gpt35_results[\"decision_index\"] // 3\n",
    "gpt35_results[\"model\"] = \"GPT-3.5\"\n",
    "gpt4_results[\"dataset\"] = gpt4_results.apply(lambda row: abbreviate(row[\"source_relation\"], row[\"target_relation\"]), axis=\"columns\")\n",
    "gpt4_results[\"experiment_run\"] = gpt4_results[\"decision_index\"] // 3\n",
    "gpt4_results[\"model\"] = \"GPT-4\"\n",
    "baseline_results[\"dataset\"] = baseline_results.apply(lambda row: abbreviate(row[\"source_relation\"], row[\"target_relation\"]), axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b53b5f-318a-4caf-8c2b-d2e43dd72079",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_results = pd.concat((gpt35_results, gpt4_results), axis=\"index\")\n",
    "\n",
    "majority_vote_df = pd.pivot(\n",
    "    llm_results.groupby(by=[\"model\", \"task_scope\", \"experiment_run\", \"dataset\", \"source\", \"target\", \"decision\"]).count()[\"benchmark\"].reset_index(),\n",
    "    index=[\"model\", \"task_scope\", \"experiment_run\", \"dataset\", \"source\", \"target\"],\n",
    "    columns=[\"decision\"],\n",
    ").reset_index()\n",
    "\n",
    "majority_vote_df.columns = [\"model\", \"task_scope\", \"experiment_run\", \"dataset\", \"source\", \"target\"] + majority_vote_df.columns.levels[1][0:3].tolist()\n",
    "for vote in [\"no\", \"unknown\", \"yes\"]:  \n",
    "    majority_vote_df[vote] = majority_vote_df[vote].fillna(0)\n",
    "\n",
    "majority_vote_df[\"decision\"] = \"unknown\"\n",
    "majority_vote_df[\"decision\"] = majority_vote_df[\"decision\"].mask(\n",
    "    majority_vote_df[\"no\"] >= 2,\n",
    "    other=\"no\",\n",
    ").mask(\n",
    "    majority_vote_df[\"yes\"] >= 2,\n",
    "    other=\"yes\",\n",
    ")\n",
    "\n",
    "majority_vote_df = majority_vote_df.merge(benchmark[[\"source\", \"target\", \"benchmark\"]], on=[\"source\", \"target\"], how=\"left\").copy()\n",
    "majority_vote_df[\"benchmark\"] = majority_vote_df[\"benchmark\"].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40237eee-8fe3-45f8-a1e1-3426b48c9ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results[\"decision\"] = \"unknown\"\n",
    "\n",
    "for dataset in baseline_results[\"dataset\"].unique():\n",
    "    _df = baseline_results.query(\"dataset == @dataset\")\n",
    "    all_f1_scores = []\n",
    "    for threshold in _df.query(\"benchmark\")[\"n-gram\"].values:\n",
    "        p, r, f1, _ = precision_recall_fscore_support(\n",
    "            _df[\"benchmark\"],\n",
    "            _df[\"n-gram\"] >= threshold,\n",
    "            average=\"binary\",\n",
    "            pos_label=True,\n",
    "            zero_division=0.0,\n",
    "        )\n",
    "        all_f1_scores.append({\n",
    "            \"threshold\": threshold,\n",
    "            \"precision\": p,\n",
    "            \"recall\": r,\n",
    "            \"f1-score\": f1,\n",
    "        })\n",
    "    all_f1_scores = pd.DataFrame(all_f1_scores)\n",
    "    best_f1 = all_f1_scores.sort_values([\"f1-score\", \"recall\"], ascending=[False, False]).iloc[0]\n",
    "    #best_f1 = all_f1_scores.loc[all_f1_scores[\"f1-score\"].argmax()]\n",
    "    baseline_results[\"decision\"] = baseline_results[\"decision\"].mask(\n",
    "        (baseline_results[\"dataset\"] == dataset) & (baseline_results[\"n-gram\"] >= best_f1[\"threshold\"]),\n",
    "        other=\"yes\",\n",
    "    ).mask(\n",
    "        (baseline_results[\"dataset\"] == dataset) & (baseline_results[\"n-gram\"] < best_f1[\"threshold\"]),\n",
    "        other=\"no\",\n",
    "    )\n",
    "\n",
    "baseline_results[\"model\"] = \"\"\n",
    "baseline_results[\"task_scope\"] = \"n-gram\"\n",
    "baseline_results[\"experiment_run\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e444c0-cf38-4930-9b2f-e86a60f671ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_decisions_df = pd.concat((majority_vote_df, baseline_results), axis=\"index\")[\n",
    "    [\"model\", \"task_scope\", \"experiment_run\", \"dataset\", \"source\", \"target\", \"decision\", \"benchmark\"]\n",
    "]\n",
    "all_decisions_df.to_csv(\"results/all_decisions_df.csv\", index=False)\n",
    "all_decisions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31083598-4de9-45bf-a8c9-13ac78b3b17b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
