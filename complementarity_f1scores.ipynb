{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df5d32-21dd-434e-a858-1a7755e0c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e444c0-cf38-4930-9b2f-e86a60f671ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_decisions_df = pd.read_csv(\"results/all_decisions_df.csv\")\n",
    "all_decisions_df.fillna({\"model\": \"\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9334dd-ccbe-460b-8df1-283b3933f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_experiments(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for model in df[\"model\"].unique():\n",
    "        model_df = df.query(\"model == @model\")\n",
    "        for dataset in df[\"dataset\"].unique():\n",
    "            dataset_df = model_df.query(\"dataset == @dataset\")\n",
    "            for task_scope in df[\"task_scope\"].unique():\n",
    "                task_scope_df = dataset_df.query(\"task_scope == @task_scope\")\n",
    "                for experiment_run in task_scope_df[\"experiment_run\"].unique():\n",
    "                    if task_scope == \"n-gram\":\n",
    "                        experiment_df = task_scope_df\n",
    "                    else:\n",
    "                        experiment_df = task_scope_df.query(\n",
    "                            \"experiment_run == @experiment_run\"\n",
    "                        )\n",
    "                    yield model, dataset, task_scope, experiment_run, experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a9fa7-f1ee-486d-b08b-81b96e54f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for model, dataset, task_scope, experiment_run, experiment_df in iterate_experiments(\n",
    "    all_decisions_df\n",
    "):\n",
    "    for (\n",
    "        other_model,\n",
    "        _,\n",
    "        other_task_scope,\n",
    "        other_experiment_run,\n",
    "        other_experiment_df,\n",
    "    ) in iterate_experiments(all_decisions_df.query(\"dataset == @dataset\")):\n",
    "        # skip inter model combinations\n",
    "        if (\n",
    "            (model == \"GPT-3.5\") and (other_model == \"GPT-4\")\n",
    "        ) or (\n",
    "            (model == \"GPT-4\") and (other_model == \"GPT-3.5\")\n",
    "        ):\n",
    "            continue\n",
    "        \n",
    "        # use a simple average when we have the same task scope twice\n",
    "        if (task_scope == other_task_scope) and (experiment_run != other_experiment_run):\n",
    "            continue\n",
    "\n",
    "        _left = experiment_df[[\"source\", \"target\", \"decision\", \"benchmark\"]]\n",
    "        _right = other_experiment_df[[\"source\", \"target\", \"decision\", \"benchmark\"]]\n",
    "        _df = _left.merge(\n",
    "            _right, on=[\"source\", \"target\"], suffixes=[\"_left\", \"_right\"], how=\"outer\"\n",
    "        )\n",
    "        unioned_decision = ((_df[\"decision_left\"] == \"yes\") | (_df[\"decision_right\"] == \"yes\"))\n",
    "        p, r, f1, _ = precision_recall_fscore_support(\n",
    "            _df[\"benchmark_left\"],\n",
    "            unioned_decision,\n",
    "            average=\"binary\",\n",
    "            pos_label=True,\n",
    "            zero_division=0.0,\n",
    "        )\n",
    "\n",
    "        scores.append(\n",
    "            {\n",
    "                \"dataset\": dataset,\n",
    "                \"from\": task_scope,\n",
    "                \"from_run\": experiment_run,\n",
    "                \"from_model\": model,\n",
    "                \"to\": other_task_scope,\n",
    "                \"to_run\": other_experiment_run,\n",
    "                \"to_model\": other_model,\n",
    "                \"precision\": p,\n",
    "                \"recall\": r,\n",
    "                \"f1-score\": f1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a4eb7-ee9a-4e52-a9a5-af8e403a4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_order = [\"n-gram\", \"1-to-1\", \"1-to-n\", \"n-to-1\", \"n-to-n\"]\n",
    "\n",
    "scores_table = pd.pivot(\n",
    "    scores_df\n",
    "    .groupby([\"dataset\", \"from\", \"to\", \"to_model\"])[[\"f1-score\", \"precision\", \"recall\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .groupby([\"from\", \"to\", \"to_model\"])[[\"f1-score\", \"precision\", \"recall\"]]\n",
    "    .mean()\n",
    "    .reset_index(),\n",
    "    index=\"from\",\n",
    "    columns=[\"to_model\", \"to\"],\n",
    "    values=[\"f1-score\", \"precision\", \"recall\"],\n",
    ").loc[\n",
    "    scope_order\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27aeac5-2e01-4da7-9e71-5381bbebcef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def should_visualize(from_scope: str, to_scope: str) -> bool:\n",
    "    \"\"\"To create a diagonal table, this method helps in deciding which cell to draw.\"\"\"\n",
    "    return scope_order.index(from_scope) <= scope_order.index(to_scope)\n",
    "\n",
    "\n",
    "columns = [[\"\"] + [\"GPT-3.5\"] * 4 + [\"GPT-4\"] * 3, scope_order + scope_order[2:]]\n",
    "values = []\n",
    "for from_scope, row in scores_table.iterrows():\n",
    "    value_row = []\n",
    "    for (model, to_scope) in zip(*columns):\n",
    "        value = scores_table.loc[from_scope, (\"f1-score\", model, to_scope)]\n",
    "        if pd.isnull(value):\n",
    "            value_row.append(pd.NA)\n",
    "            continue\n",
    "\n",
    "        if not should_visualize(from_scope, to_scope):\n",
    "            value_row.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        if from_scope == \"n-gram\":\n",
    "            reference_value = scores_table.loc[from_scope, (\"f1-score\", \"\", from_scope)]\n",
    "        else:\n",
    "            reference_value = scores_table.loc[from_scope, (\"f1-score\", model, from_scope)]\n",
    "        value_row.append(\n",
    "            scores_table.loc[from_scope, (\"f1-score\", model, to_scope)] - reference_value\n",
    "        )\n",
    "    values.append(value_row)\n",
    "        \n",
    "texts = [\n",
    "    [\n",
    "        (\n",
    "            f\"{scores_table.loc[from_scope, ('f1-score', model, to_scope)]:.3f} \"\n",
    "            f\"({scores_table.loc[from_scope, ('precision', model, to_scope)]:.2f}, \"\n",
    "            f\"{scores_table.loc[from_scope, ('recall', model, to_scope)]:.2f})\"\n",
    "        ) if should_visualize(from_scope, to_scope) else \"\"\n",
    "        for (model, to_scope) in zip(*columns)\n",
    "    ]\n",
    "    for from_scope, row in scores_table.iterrows()\n",
    "]\n",
    "\n",
    "fig = go.Figure(\n",
    "    layout=dict(\n",
    "        title=\"Median F1-scores compared to baseline (green: better, purple: worse)\",\n",
    "        height=600,\n",
    "        width=1000,\n",
    "        yaxis={\"autorange\": \"reversed\"}\n",
    "    ),\n",
    "    data=go.Heatmap(\n",
    "        x=columns,\n",
    "        y=scores_table.index,\n",
    "        z=values,\n",
    "        text=texts,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 12},\n",
    "        colorscale=\"PrGn\",\n",
    "        zmin=-0.5,\n",
    "        zmax=0.5,\n",
    "        showscale=False,\n",
    "    ),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a189b-e49b-45db-957d-7ef186a03006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04193e77-e234-4dcb-8026-df34ce0859dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
