{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df5d32-21dd-434e-a858-1a7755e0c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e444c0-cf38-4930-9b2f-e86a60f671ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_decisions_df = pd.read_csv(\"results/all_decisions_df.csv\")\n",
    "all_decisions_df.fillna({\"model\": \"\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9334dd-ccbe-460b-8df1-283b3933f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_experiments(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for model in df[\"model\"].unique():\n",
    "        model_df = df.query(\"model == @model\")\n",
    "        for dataset in df[\"dataset\"].unique():\n",
    "            dataset_df = model_df.query(\"dataset == @dataset\")\n",
    "            for task_scope in df[\"task_scope\"].unique():\n",
    "                task_scope_df = dataset_df.query(\"task_scope == @task_scope\")\n",
    "                for experiment_run in task_scope_df[\"experiment_run\"].unique():\n",
    "                    if task_scope == \"n-gram\":\n",
    "                        experiment_df = task_scope_df\n",
    "                    else:\n",
    "                        experiment_df = task_scope_df.query(\n",
    "                            \"experiment_run == @experiment_run\"\n",
    "                        )\n",
    "                    yield model, dataset, task_scope, experiment_run, experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a9fa7-f1ee-486d-b08b-81b96e54f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "ignore_datasets_df = all_decisions_df.copy()\n",
    "ignore_datasets_df[\"dataset\"] = \"all\"\n",
    "\n",
    "for model, _, task_scope, experiment_run, experiment_df in iterate_experiments(\n",
    "    ignore_datasets_df\n",
    "):\n",
    "    _scope_TP = set(\n",
    "        (p[0], p[1])\n",
    "        for p in experiment_df.query(\"(decision == 'yes') and benchmark\")[\n",
    "            [\"source\", \"target\"]\n",
    "        ].values\n",
    "    )\n",
    "    for (\n",
    "        other_model,\n",
    "        _,\n",
    "        other_task_scope,\n",
    "        other_experiment_run,\n",
    "        other_experiment_df,\n",
    "    ) in iterate_experiments(ignore_datasets_df):\n",
    "        # skip inter model combinations\n",
    "        if (\n",
    "            (model == \"GPT-3.5\") and (other_model == \"GPT-4\")\n",
    "        ) or (\n",
    "            (model == \"GPT-4\") and (other_model == \"GPT-3.5\")\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        # use a simple average when we have the same task scope twice\n",
    "        if (task_scope == other_task_scope) and (experiment_run != other_experiment_run):\n",
    "            continue\n",
    "\n",
    "        # get the true positives for the other task scope\n",
    "        _other_scope_TP = set(\n",
    "            (p[0], p[1])\n",
    "            for p in other_experiment_df.query(\"(decision == 'yes') and benchmark\")[\n",
    "                [\"source\", \"target\"]\n",
    "            ].values\n",
    "        )\n",
    "\n",
    "        combinations.append(\n",
    "            {\n",
    "                \"from\": task_scope,\n",
    "                \"from_run\": experiment_run,\n",
    "                \"from_model\": model,\n",
    "                \"from_TP\": len(_scope_TP),\n",
    "                \"to\": other_task_scope,\n",
    "                \"to_run\": other_experiment_run,\n",
    "                \"to_model\": other_model,\n",
    "                \"to_TP\": len(_other_scope_TP),\n",
    "                \"comb_TP\": len(_scope_TP | _other_scope_TP),\n",
    "            }\n",
    "        )\n",
    "\n",
    "combinations_df = pd.DataFrame(combinations)\n",
    "combinations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a4eb7-ee9a-4e52-a9a5-af8e403a4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_order = [\"n-gram\", \"1-to-1\", \"1-to-n\", \"n-to-1\", \"n-to-n\"]\n",
    "\n",
    "combination_table = pd.pivot(\n",
    "    combinations_df\n",
    "    .groupby([\"from\", \"to\", \"to_model\"])[\"comb_TP\"]\n",
    "    .mean()\n",
    "    .reset_index(),\n",
    "    index=\"from\",\n",
    "    columns=[\"to_model\", \"to\"],\n",
    "    values=\"comb_TP\",\n",
    ").loc[\n",
    "    scope_order,\n",
    "    list(zip([\"\"] + [\"GPT-3.5\"] * 4 + [\"GPT-4\"] * 3, scope_order + scope_order[2:]))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27aeac5-2e01-4da7-9e71-5381bbebcef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def should_visualize(from_scope: str, to_scope: str) -> bool:\n",
    "    \"\"\"TO create a diagonal table, this method helps in deciding which cell to draw.\"\"\"\n",
    "    return scope_order.index(from_scope) <= scope_order.index(to_scope)\n",
    "\n",
    "\n",
    "values = [\n",
    "    [\n",
    "        value if (from_scope != to_scope) and should_visualize(from_scope, to_scope) else 0.0\n",
    "        for (model, to_scope), value in row.items()\n",
    "    ]\n",
    "    for from_scope, row in combination_table.iterrows()\n",
    "]\n",
    "texts = [\n",
    "    [\n",
    "        f\"{value:.1f}\" if should_visualize(from_scope, to_scope) else \"\"\n",
    "        for (model, to_scope), value in row.items()\n",
    "    ]\n",
    "    for from_scope, row in combination_table.iterrows()\n",
    "]\n",
    "\n",
    "fig = go.Figure(\n",
    "    layout=dict(\n",
    "        title=\"Median F1-scores compared to baseline (green: better, purple: worse)\",\n",
    "        height=600,\n",
    "        width=1000,\n",
    "        yaxis={\"autorange\": \"reversed\"}\n",
    "    ),\n",
    "    data=go.Heatmap(\n",
    "        x=[combination_table.columns.get_level_values(0), combination_table.columns.get_level_values(1)],\n",
    "        y=combination_table.index,\n",
    "        z=values,\n",
    "        text=texts,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 16},\n",
    "        colorscale=\"greens\",\n",
    "        zmin=0,\n",
    "        zmax=combination_table.max().max(),\n",
    "        showscale=False,\n",
    "    ),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a189b-e49b-45db-957d-7ef186a03006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
